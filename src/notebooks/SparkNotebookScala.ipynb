{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f364ce3e",
   "metadata": {},
   "source": [
    "### Notebook - Running Spark on OCI Dataflow using Scala\n",
    "<details>\n",
    "<summary><font size=\"2\">Check OCI Dataflow documentation for policies required;</font></summary>\n",
    "<li><a href=\"https://docs.public.oneportal.content.oci.oraclecloud.com/en-us/iaas/data-flow/using/policies-data-flow-studio.htm#policies-data-flow-studio\">Policies Required to Integrate Data Flow and Data Science</a></li>\n",
    "</details>",
    "<details>",
    "<summary><font size=\"2\">Check that the data science notebook session has privileges on OCI Dataflow family and any dependencies;</font></summary>\n",
    "\n",
    "```ALL {resource.type = 'datasciencenotebooksession', resource.compartment.id = '<compartment_id>'}```\n",
    "\n",
    "</details>",
    "<details>",
    "<summary><font size=\"2\">Policy for Data Science to manage OCI Data Flow</font></summary>\n",
    "\n",
    "```allow dynamic-group <ds-dynamic-group> to manage dataflow-family in compartment '<your-compartment-name>'```\n",
    "\n",
    "</details>",
    "<details>",
    "<summary><font size=\"2\">Policy statements for logging</font></summary>\n",
    "\n",
    "```allow dynamic-group <ds-dynamic-group> to manage log-groups in compartment '<your-compartment-name>'```\n",
    "```allow dynamic-group <ds-dynamic-group> to manage log-content in compartment '<your-compartment-name>'```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1eb2a1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ads\n",
    "ads.set_auth(\"resource_principal\") # Supported values: resource_principal, api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ab91a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your values;\n",
    "#  compartment id for creating the compute/application in OCI Dataflow\n",
    "#  bucket for writing OCI Dataflow logs\n",
    "#  namespace your OCI Object Storage namespace\n",
    "#  driver_shape ie. VM.Standard.E4.Flex\n",
    "#  executor_shape ie. VM.Standard.E4.Flex\n",
    "# \n",
    "#\n",
    "#Non- autoscaling create session command in New Compute - this will print a session id that you could reconnect to  \n",
    "import json\n",
    "command = {\n",
    "    \"compartmentId\": \"<your_compartment_id>\",\n",
    "    \"displayName\": \"Data Analysis in Scala\",\n",
    "    \"language\": \"SCALA\",\n",
    "    \"sparkVersion\": \"3.2.1\",\n",
    "    \"driverShape\": \"<driver_shape>\",\n",
    "    \"executorShape\": \"<executor_shape>\",\n",
    "    \"driverShapeConfig\":{\"ocpus\":1,\"memoryInGBs\":16},\n",
    "    \"executorShapeConfig\":{\"ocpus\":1,\"memoryInGBs\":16},\n",
    "    \"numExecutors\": 1,\n",
    "    \"type\": \"SESSION\",\n",
    "    \"logsBucketUri\": \"oci://<bucket>@<namespace>/\",\n",
    "    \"configuration\": {\"dataflow.auth\":\"resource_principal\", \"spark.oracle.datasource.enabled\":\"true\", \"spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version\": \"2\", \"spark.executorEnv.OCI_JAVASDK_JERSEY_CLIENT_DEFAULT_CONNECTOR_ENABLED\": \"true\", \"spark.driverEnv.OCI_JAVASDK_JERSEY_CLIENT_DEFAULT_CONNECTOR_ENABLED\": \"true\"}\n",
    "}\n",
    "command = f'\\'{json.dumps(command)}\\''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72c98452",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_ext dataflow.magics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f61e3cf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<table>\n",
       "  <tr>\n",
       "    <th style=\"text-align:left\">Magic</th>\n",
       "    <th style=\"text-align:left\">Example</th>\n",
       "    <th style=\"text-align:left\">Explanation</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>create_session</td>\n",
       "    <td  style=\"text-align:left\">%create_session -l python -c '{\"compartmentId\":\"Data Flow Run resource compartment OCID\",\"displayName\":\"SessionApp\",\"sparkVersion\":\"3.2.1\",\"driverShape\":\"VM.Standard2.1\",\"executorShape\":\"VM.Standard2.1\",\"numExecutors\":1,\"archiveUri\":\"Object Storage URL for Data Flow zip archive.\",\"metastoreId\":\"optional metastore OCID\",\"configuration\":{      \"spark.archives\":\"oci://bucket@namespace/path/to/conda/pack\",      #optional property to use Dataflow 'Run' resource to access OCI resources.\n",
       "      \"dataflow.auth\":\"resource_principal\"   }}'</td>\n",
       "    <td style=\"text-align:left\">Creates new session by providing session details.<br/><br/><b>Example command for Flex shapes :</b><br/>\n",
       "    %create_session -l python -c '{\"compartmentId\":\"Data Flow Run resource compartment OCID\",\"displayName\":\"SessionApp\",\"sparkVersion\":\"3.2.1\",\"driverShape\":\"VM.Standard.E4.Flex\",\"executorShape\":\"VM.Standard.E4.Flex\",\"numExecutors\":1,\"driverShapeConfig\":{\"ocpus\":1,\"memoryInGBs\":16},\"executorShapeConfig\":{\"ocpus\":1,\"memoryInGBs\":16}}'\n",
       "    <br/><br/><b>Example command for Spark dynamic allocation :</b><br/>\n",
       "    %create_session -l python -c '{\"compartmentId\":\"Data Flow Run resource compartment OCID\",\"displayName\":\"SessionApp\",\"sparkVersion\":\"3.2.1\",\"driverShape\":\"VM.Standard2.1\",\"executorShape\":\"VM.Standard2.1\",\"numExecutors\":1,\"configuration\":{ \"spark.dynamicAllocation.enabled\":\"true\", \"spark.dynamicAllocation.shuffleTracking.enabled\":\"true\", \"spark.dynamicAllocation.minExecutors\":\"1\", \"spark.dynamicAllocation.maxExecutors\":\"4\", \"spark.dynamicAllocation.executorIdleTimeout\":\"60\", \"spark.dynamicAllocation.schedulerBacklogTimeout\":\"60\", \"spark.dataflow.dynamicAllocation.quotaPolicy\":\"min\" }}'\n",
       "    </td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>activate_session</td>\n",
       "    <td  style=\"text-align:left\">%activate_session -l python -c '{\"compartmentId\":\"Data Flow Run resource compartment OCID\",\"displayName\":\"SessionApp\",\"applicationId\":\"Existing sessionId to activate.\"}'</td>\n",
       "    <td  style=\"text-align:left\">Activate session by providing existing sessionId.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>use_session</td>\n",
       "    <td  style=\"text-align:left\">%use_session -s {sessionId}</td>\n",
       "    <td  style=\"text-align:left\">To use already existing active session.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>status</td>\n",
       "    <td  style=\"text-align:left\">%status</td>\n",
       "    <td  style=\"text-align:left\">Outputs current session status.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>update_session</td>\n",
       "    <td  style=\"text-align:left\">%update_session -i '{\"maxDurationInMinutes\": 4896,\"idleTimeoutInMinutes\": 4888}'</td>\n",
       "    <td  style=\"text-align:left\">Updates current active session[not session config] for max duration or idle time out.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>stop_session</td>\n",
       "    <td  style=\"text-align:left\">%stop_session</td>\n",
       "    <td  style=\"text-align:left\">Stops current active session. One active session should be associated with current notebook to stop.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>config</td>\n",
       "    <td  style=\"text-align:left\">%config</td>\n",
       "    <td  style=\"text-align:left\">Outputs current session configuration.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>configure_session</td>\n",
       "    <td  style=\"text-align:left\">%configure_session -i '{\"driverShape\": \"VM.Standard2.1\", \"executorShape\": \"VM.Standard2.1\", \"numExecutors\": 1}'</td>\n",
       "    <td  style=\"text-align:left\">Configures the session creation parameters. The force flag -f is mandatory for immediate effect of the config change, in that case session will be dropped and recreated.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>spark</td>\n",
       "    <td  style=\"text-align:left\">%%spark -o df<br/>df = spark.read.parquet('...</td>\n",
       "    <td  style=\"text-align:left\">Executes spark commands.<br/>\n",
       "    Parameters:\n",
       "      <ul>\n",
       "        <li>-o VAR_NAME: The Spark dataframe of name VAR_NAME will be available in the %%local Python context as a\n",
       "          <a href=\"http://pandas.pydata.org/\">Pandas</a> dataframe with the same name.</li>\n",
       "        <li>-m METHOD: Sample method, either <tt>take</tt> or <tt>sample</tt>.</li>\n",
       "        <li>-n MAXROWS: The maximum number of rows of a dataframe that will be pulled from Livy to Jupyter.\n",
       "            If this number is negative, then the number of rows will be unlimited.</li>\n",
       "        <li>-r FRACTION: Fraction used for sampling.</li>\n",
       "      </ul>\n",
       "    </td>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e8e1bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up the Cluster..\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4555c55e4d24e99a78c6cffb3e32967",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster is ready..\n",
      "Starting Spark application..\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>Session ID</th><th>Kind</th><th>State</th><th>Current session</th></tr><tr><td>ocid1.dataflowapplication.oc1.iad.anuwcljtnif7xwia2wvhumxogghuzyd4u2smgmdwy6nrjefifkc7l2lxbzua</td><td>spark</td><td>IN_PROGRESS</td><td><a target=\"_blank\" href=\"https://console.us-phoenix-1.oraclecloud.com/data-flow/runs/details/ocid1.dataflowrun.oc1.iad.anuwcljtnif7xwiayi6gwsonppo3qh4dc5wa5hrtkgbqzhvvh6ti4q44c2gq?region=us-ashburn-1\">Dataflow Run</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "SparkContext available as 'sc'.\n"
     ]
    }
   ],
   "source": [
    "%create_session -l scala -c $command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c225e0fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defined class Employee\n",
      "df: org.apache.spark.sql.DataFrame = [id: int, name: string]\n"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "case class Employee(id: Int, name: String)\n",
    "val df = Seq(new Employee(1, \"Elia\"), new Employee(2, \"Teo\"), new Employee(3, \"Fang\")).toDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c5eb7f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|name|\n",
      "+---+----+\n",
      "|  1|Elia|\n",
      "|  2| Teo|\n",
      "|  3|Fang|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ee18b32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8dc6fcf241f40a69d200534891b1a25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session has been stopped successfully.\n"
     ]
    }
   ],
   "source": [
    "%stop_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debb35c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyspark32_p38_cpu_v2]",
   "language": "python",
   "name": "conda-env-pyspark32_p38_cpu_v2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
