{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96e84d8d",
   "metadata": {},
   "source": [
    "### Notebook - Running Spark on OCI Dataflow using PySpark\n",
    "<details>\n",
    "<summary><font size=\"2\">Check OCI Dataflow documentation for policies required;</font></summary>\n",
    "<li><a href=\"https://docs.public.oneportal.content.oci.oraclecloud.com/en-us/iaas/data-flow/using/policies-data-flow-studio.htm#policies-data-flow-studio\">Policies Required to Integrate Data Flow and Data Science</a></li>\n",
    "</details>",
    "<details>",
    "<summary><font size=\"2\">Check that the data science notebook session has privileges on OCI Dataflow family and any dependencies;</font></summary>\n",
    "```ALL {resource.type = 'datasciencenotebooksession', resource.compartment.id = '<compartment_id>'}\n",
    "```\n",
    "</details>",
    "<details>",
    "<summary><font size=\"2\">Policies</font></summary>\n",
    "```allow dynamic-group <ds-dynamic-group> to manage dataflow-family in compartment '<your-compartment-name>'\n",
    "```\n",
    "</details>",
    "<details>",
    "<summary><font size=\"2\">Policies for logging</font></summary>\n",
    "```allow dynamic-group <ds-dynamic-group> to manage log-groups in compartment '<your-compartment-name>' \n",
    "allow dynamic-group <ds-dynamic-group> to manage log-content in compartment '<your-compartment-name>'\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1be8bb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ads\n",
    "ads.set_auth(\"resource_principal\") # Supported values: resource_principal, api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40dbe383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your values;\n",
    "#  compartment id for creating the compute/application in OCI Dataflow\n",
    "#  bucket for writing OCI Dataflow logs\n",
    "#  namespace your OCI Object Storage namespace\n",
    "#  driver_shape ie. VM.Standard.E4.Flex\n",
    "#  executor_shape ie. VM.Standard.E4.Flex\n",
    "# \n",
    "#\n",
    "#Non- autoscaling create session command in New Compute - this will print a session id that you could reconnect to  \n",
    "import json\n",
    "command = {\n",
    "    \"compartmentId\": \"<your_compartment_id>\",\n",
    "    \"displayName\": \"Data Analysis in Python\",\n",
    "    \"language\": \"PYTHON\",\n",
    "    \"sparkVersion\": \"3.2.1\",\n",
    "    \"driverShape\": \"<driver_shape>\",\n",
    "    \"executorShape\": \"<executor_shape>\",\n",
    "    \"driverShapeConfig\":{\"ocpus\":1,\"memoryInGBs\":16},\n",
    "    \"executorShapeConfig\":{\"ocpus\":1,\"memoryInGBs\":16},\n",
    "    \"numExecutors\": 1,\n",
    "    \"type\": \"SESSION\",\n",
    "    \"logsBucketUri\": \"oci://<bucket>@<namespace>/\",\n",
    "    \"configuration\": {\"spark.oracle.datasource.enabled\":\"true\"}\n",
    "}\n",
    "command = f'\\'{json.dumps(command)}\\''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54c27152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataflow.magics extension is already loaded. To reload it, use:\n",
      "  %reload_ext dataflow.magics\n"
     ]
    }
   ],
   "source": [
    "load_ext dataflow.magics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21ecbe92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<table>\n",
       "  <tr>\n",
       "    <th style=\"text-align:left\">Magic</th>\n",
       "    <th style=\"text-align:left\">Example</th>\n",
       "    <th style=\"text-align:left\">Explanation</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>create_session</td>\n",
       "    <td  style=\"text-align:left\">%create_session -l python -c '{\"compartmentId\":\"Data Flow Run resource compartment OCID\",\"displayName\":\"SessionApp\",\"sparkVersion\":\"3.2.1\",\"driverShape\":\"VM.Standard2.1\",\"executorShape\":\"VM.Standard2.1\",\"numExecutors\":1,\"archiveUri\":\"Object Storage URL for Data Flow zip archive.\",\"metastoreId\":\"optional metastore OCID\",\"configuration\":{      \"spark.archives\":\"oci://bucket@namespace/path/to/conda/pack\",      #optional property to use Dataflow 'Run' resource to access OCI resources.\n",
       "      \"dataflow.auth\":\"resource_principal\"   }}'</td>\n",
       "    <td style=\"text-align:left\">Creates new session by providing session details.<br/><br/><b>Example command for Flex shapes :</b><br/>\n",
       "    %create_session -l python -c '{\"compartmentId\":\"Data Flow Run resource compartment OCID\",\"displayName\":\"SessionApp\",\"sparkVersion\":\"3.2.1\",\"driverShape\":\"VM.Standard.E4.Flex\",\"executorShape\":\"VM.Standard.E4.Flex\",\"numExecutors\":1,\"driverShapeConfig\":{\"ocpus\":1,\"memoryInGBs\":16},\"executorShapeConfig\":{\"ocpus\":1,\"memoryInGBs\":16}}'\n",
       "    <br/><br/><b>Example command for Spark dynamic allocation :</b><br/>\n",
       "    %create_session -l python -c '{\"compartmentId\":\"Data Flow Run resource compartment OCID\",\"displayName\":\"SessionApp\",\"sparkVersion\":\"3.2.1\",\"driverShape\":\"VM.Standard2.1\",\"executorShape\":\"VM.Standard2.1\",\"numExecutors\":1,\"configuration\":{ \"spark.dynamicAllocation.enabled\":\"true\", \"spark.dynamicAllocation.shuffleTracking.enabled\":\"true\", \"spark.dynamicAllocation.minExecutors\":\"1\", \"spark.dynamicAllocation.maxExecutors\":\"4\", \"spark.dynamicAllocation.executorIdleTimeout\":\"60\", \"spark.dynamicAllocation.schedulerBacklogTimeout\":\"60\", \"spark.dataflow.dynamicAllocation.quotaPolicy\":\"min\" }}'\n",
       "    </td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>activate_session</td>\n",
       "    <td  style=\"text-align:left\">%activate_session -l python -c '{\"compartmentId\":\"Data Flow Run resource compartment OCID\",\"displayName\":\"SessionApp\",\"applicationId\":\"Existing sessionId to activate.\"}'</td>\n",
       "    <td  style=\"text-align:left\">Activate session by providing existing sessionId.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>use_session</td>\n",
       "    <td  style=\"text-align:left\">%use_session -s {sessionId}</td>\n",
       "    <td  style=\"text-align:left\">To use already existing active session.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>status</td>\n",
       "    <td  style=\"text-align:left\">%status</td>\n",
       "    <td  style=\"text-align:left\">Outputs current session status.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>update_session</td>\n",
       "    <td  style=\"text-align:left\">%update_session -i '{\"maxDurationInMinutes\": 4896,\"idleTimeoutInMinutes\": 4888}'</td>\n",
       "    <td  style=\"text-align:left\">Updates current active session[not session config] for max duration or idle time out.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>stop_session</td>\n",
       "    <td  style=\"text-align:left\">%stop_session</td>\n",
       "    <td  style=\"text-align:left\">Stops current active session. One active session should be associated with current notebook to stop.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>config</td>\n",
       "    <td  style=\"text-align:left\">%config</td>\n",
       "    <td  style=\"text-align:left\">Outputs current session configuration.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>configure_session</td>\n",
       "    <td  style=\"text-align:left\">%configure_session -i '{\"driverShape\": \"VM.Standard2.1\", \"executorShape\": \"VM.Standard2.1\", \"numExecutors\": 1}'</td>\n",
       "    <td  style=\"text-align:left\">Configures the session creation parameters. The force flag -f is mandatory for immediate effect of the config change, in that case session will be dropped and recreated.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>spark</td>\n",
       "    <td  style=\"text-align:left\">%%spark -o df<br/>df = spark.read.parquet('...</td>\n",
       "    <td  style=\"text-align:left\">Executes spark commands.<br/>\n",
       "    Parameters:\n",
       "      <ul>\n",
       "        <li>-o VAR_NAME: The Spark dataframe of name VAR_NAME will be available in the %%local Python context as a\n",
       "          <a href=\"http://pandas.pydata.org/\">Pandas</a> dataframe with the same name.</li>\n",
       "        <li>-m METHOD: Sample method, either <tt>take</tt> or <tt>sample</tt>.</li>\n",
       "        <li>-n MAXROWS: The maximum number of rows of a dataframe that will be pulled from Livy to Jupyter.\n",
       "            If this number is negative, then the number of rows will be unlimited.</li>\n",
       "        <li>-r FRACTION: Fraction used for sampling.</li>\n",
       "      </ul>\n",
       "    </td>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95468dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up the Cluster..\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c75cfcbf20f4d7eaaf0971fb9a5aaec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster is ready..\n",
      "Starting Spark application..\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>Session ID</th><th>Kind</th><th>State</th><th>Current session</th></tr><tr><td>ocid1.dataflowapplication.oc1.iad.anuwcljrnif7xwia55gckrekr54sfmemhixprzag7g2ecvqkqym4ea546raq</td><td>pyspark</td><td>IN_PROGRESS</td><td><a target=\"_blank\" href=\"https://console.us-phoenix-1.oraclecloud.com/data-flow/runs/details/ocid1.dataflowrun.oc1.iad.anuwcljrnif7xwiadsxteuu5uwb5z5wjaxxbberdl5rc4bqu4c5rg4ompguq?region=us-ashburn-1\">Dataflow Run</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "SparkContext available as 'sc'.\n"
     ]
    }
   ],
   "source": [
    "create_session -l python -c $command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1830940c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+-------+----+----------+-----+\n",
      "|rank|      city|  state|code|population|price|\n",
      "+----+----------+-------+----+----------+-----+\n",
      "| 295|South Bend|Indiana|  IN|    101190|112.9|\n",
      "+----+----------+-------+----+----------+-----+"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "data = [[295, \"South Bend\", \"Indiana\", \"IN\", 101190, 112.9]]\n",
    "columns = [\"rank\", \"city\", \"state\", \"code\", \"population\", \"price\"]\n",
    "\n",
    "df1 = spark.createDataFrame(data, schema=\"rank LONG, city STRING, state STRING, code STRING, population LONG, price DOUBLE\")\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e774e03c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bad972a11d4b40338b10aa3514072c82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session has been stopped successfully.\n"
     ]
    }
   ],
   "source": [
    "%stop_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0698fa3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyspark32_p38_cpu_v2]",
   "language": "python",
   "name": "conda-env-pyspark32_p38_cpu_v2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
